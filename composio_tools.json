event: message
data: {"result":{"tools":[{"name":"COMPOSIO_CHECK_ACTIVE_CONNECTION","description":"Deprecated: use check active connections instead for bulk operations. check active connection status for a toolkit or specific connected account id. returns connection details if active, or required parameters for establishing connection if none exists. active connections enable agent actions on the toolkit.","inputSchema":{"type":"object","properties":{"connected_account_id":{"type":"string","description":"Specific connected account ID to check status for"},"toolkit":{"type":"string","description":"Name of the toolkit to check\nExamples:\n  \"github\"\n  \"slack\"\n  \"hubspot\""}},"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Check active connection (deprecated)","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_CHECK_ACTIVE_CONNECTIONS","description":"Check active connection status for multiple toolkits or specific connected account ids. returns connection details if active, or required parameters for establishing connection if none exists. active connections enable agent actions on toolkits.","inputSchema":{"type":"object","properties":{"requests":{"type":"array","items":{"type":"object","properties":{"connected_account_id":{"type":"string","description":"Specific connected account ID to check status for\nExample: \"ca_1234567890\""},"toolkit":{"type":"string","description":"Name of the toolkit to check\nExamples:\n  \"github\"\n  \"slack\"\n  \"hubspot\""}},"additionalProperties":false,"description":"CheckConnectionRequest"},"minItems":1,"description":"List of connection check requests"}},"required":["requests"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Check multiple active connections","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_CREATE_PLAN","description":"\nThis is a workflow builder that ensures the LLM produces a complete, step-by-step plan for any use case.\nWHEN TO CALL:\n- Call this tool based on COMPOSIO_SEARCH_TOOLS output. If search tools response indicates create_plan should be called and the usecase is not easy, call it.\n- Use this tool after COMPOSIO_SEARCH_TOOLS or COMPOSIO_MANAGE_CONNECTIONS to generate an execution plan for the user's use case.\n- USE for medium or hard tasks — skip it for easy ones.\n- If the user switches to a new use case in the same chat and COMPOSIO_SEARCH_TOOLS again instructs you to call the planner, you MUST call this tool again for that new use case.\n\nMemory Integration:\n- You can choose to add the memory received from the search tool into the known_fields parameter of the plan function to enhance planning with discovered relationships and information.\n\nOutputs a complete plan with sections such as \"workflow_steps\", \"complexity_assessment\", \"decision_matrix\", \"failure_handling\" \"output_format\", and more as needed.\n\nIf you skip this step for non-easy tasks, workflows will likely be incomplete, or fail during execution for complex tasks.\nCalling it guarantees reliable, accurate, and end-to-end workflows aligned with the available tools and connections.\n    ","inputSchema":{"type":"object","properties":{"difficulty":{"type":"string","enum":["medium","hard"],"description":"Difficulty level for the plan. Choose \"medium\" for moderate complexity (summarize slack messages from last day), and \"hard\" for complex tasks requiring multiple steps or advanced logic (create personalized draft for 100 emails). Do not call for easy tasks."},"known_fields":{"type":"string","description":"Provide any workflow inputs you already know as comma-separated key:value pairs (not an array). E.g. channel name, user email, timezone, etc. This helps the tool infer or look up relevant memories (like resoliving channel_id from a given channel_name). Keep max 2-3 short and structured values— focus on stable identifiers, names, emails, or settings only. Do not include free-form or long text (like messages, notes, or descriptions). Example: \"channel_name:pod-sdk, channel_id:123, user_names:John,Maria, timezone:Asia/Kolkata\""},"primary_tool_slugs":{"type":"array","items":{"type":"string"},"description":"List of primary tool slugs that can accomplish the main task. Never invent tool slugs, only use the ones given by Search. For example: ['GITHUB_LIST_PULL_REQUESTS', 'SLACK_SEND_MESSAGE']\nExamples:\n  [\"GITHUB_LIST_PULL_REQUESTS\",\"SLACK_SEND_MESSAGE\"]\n  [\"GMAIL_SEND_EMAIL\"]"},"reasoning":{"type":"string","description":"Short reasoning from the search about the use case and how the selected tools can accomplish it"},"related_tool_slugs":{"type":"array","items":{"type":"string"},"description":"List of related/supporting tool slugs that might be useful. These are optional tools that could help with the task. Never invent tool slugs, only use the ones given by Search.\nExamples:\n  [\"GITHUB_GET_REPOSITORY\",\"SLACK_LIST_CHANNELS\"]\n  []","default":[]},"use_case":{"type":"string","description":"Detailed explanation of the use case the user is trying to accomplish. Include as many details as possible for a better plan"},"session_id":{"type":"string","description":"ALWAYS pass the session_id that was provided in the SEARCH_TOOLS response."}},"required":["difficulty","known_fields","primary_tool_slugs","reasoning","use_case"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Create Plan","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_DOWNLOAD_S3_FILE","description":"Download a file from a public s3 (or r2) url to a local path.","inputSchema":{"type":"object","properties":{"local_path":{"type":["string","null"],"default":null,"description":"Optional local path where the file should be saved. If not provided, will use a temporary directory with the filename from the URL\nExamples:\n  \"/tmp/downloaded_file.pdf\"\n  \"./downloads/document.docx\"\n  \"~/Documents/file.txt\""},"s3_url":{"type":"string","description":"Public S3 URL to download the file from\nExamples:\n  \"https://pub-b71cb36a6853407fa468c5d6dec16633.r2.dev/44137/googledrive/DOWNLOAD_FILE/response/6a2edb7a4521dfc57f5ee3bf06441f1a\"\n  \"https://bucket-name.s3.amazonaws.com/path/to/file.pdf\""}},"required":["s3_url"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Download S3 File","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_ENABLE_TRIGGER","description":"Enable a specific trigger for the authenticated user.","inputSchema":{"type":"object","properties":{"config_params":{"type":"object","properties":{},"additionalProperties":true,"description":"Configuration parameters for the trigger","default":{}},"connected_account_id":{"type":"string","description":"Connected account ID to enable trigger for"},"toolkit_slug":{"type":"string","description":"Slug of the toolkit\nExample: \"gmail\""},"trigger_name":{"type":"string","description":"Name of the trigger to enable\nExample: \"GMAIL_NEW_GMAIL_MESSAGE\""},"user_id":{"type":"string","description":"User ID for the trigger","default":"default"}},"required":["connected_account_id","toolkit_slug","trigger_name"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Enable trigger","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_EXECUTE_RECIPE","description":"Executes a Recipe","inputSchema":{"type":"object","properties":{"recipe_id":{"type":"string","description":"Recipe id to update (optional). If not provided, will create a new recipe\nExample: \"rcp_rBvLjfof_THF\""},"input_data":{"type":"object","properties":{},"additionalProperties":false,"description":"Input object to pass to the Recipe"}},"required":["recipe_id","input_data"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Execute Recipe","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_EXECUTE_TOOL","description":"Execute a tool using the composio api.","inputSchema":{"type":"object","properties":{"allow_destructive":{"type":"boolean","description":"Whether to allow destructive tools to be executed. If true, the tool will be executed even if it is destructive.","default":false},"arguments":{"type":"object","properties":{},"additionalProperties":true,"description":"The arguments to be passed to the tool. The schema of the arguments is present in the retrieve_actions response\nExamples:\n  {\"body\":\"This is a test\",\"subject\":\"Hello\",\"to\":\"test@gmail.com\"}\n  {\"channel\":\"#general\",\"text\":\"Hello from Composio!\"}\n  {\"body\":\"Description of the issue\",\"labels\":[\"bug\"],\"title\":\"Bug Report\"}"},"connected_account_id":{"type":["string","null"],"default":null,"description":"The ID of the connected account to use. If not provided, uses the first active connection for the toolkit"},"tool_slug":{"type":"string","minLength":1,"description":"The slug of the tool to execute, to be used from the list of tools retrieved using retrieve_actions\nExamples:\n  \"GMAIL_SEND_EMAIL\"\n  \"SLACK_SEND_MESSAGE\"\n  \"GITHUB_CREATE_ISSUE\""}},"required":["arguments","tool_slug"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Execute Composio Tool","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":true,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_GET_DEPENDENCY_GRAPH","description":"Get the dependency graph for a given tool, showing related parent tools that might be useful. this action calls the composio labs dependency graph api to retrieve tools that are commonly used together with or before the specified tool. this helps discover related tools and understand common workflows.","inputSchema":{"type":"object","properties":{"tool_name":{"type":"string","description":"The name of the tool to get dependency graph for\nExamples:\n  \"REDDIT_SEARCH_ACROSS_SUBREDDITS\"\n  \"GITHUB_CREATE_ISSUE\"\n  \"GMAIL_SEND_EMAIL\""}},"required":["tool_name"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Get Tool Dependency Graph","scopes":[],"readOnlyHint":false,"openWorldHint":false,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_GET_RECIPE_DETAILS","description":"\n    Get the details of the existing recipe for a given recipe id.\n    ","inputSchema":{"type":"object","properties":{"recipe_id":{"type":"string","description":"Recipe id to update (optional). If not provided, will create a new recipe\nExample: \"rcp_rBvLjfof_THF\""}},"required":["recipe_id"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Get Existing Recipe Details","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_GET_REQUIRED_PARAMETERS","description":"Gets the required parameters for connecting to a toolkit via initiate connection. returns the exact parameter names and types needed for initiate connection's parameters field. supports api keys, oauth credentials, connection fields, and hybrid authentication scenarios. if has default credentials is true, you can call initiate connection with empty parameters for seamless oauth flow.","inputSchema":{"type":"object","properties":{"toolkit":{"type":"string","description":"Name of the toolkit to analyze for authentication requirements. Returns parameters for API keys, OAuth credentials, or connection fields needed by initiate_connection.\nExamples:\n  \"github\"\n  \"slack\"\n  \"hubspot\"\n  \"exa\"\n  \"zendesk\"\n  \"sharepoint\""}},"required":["toolkit"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Get required parameters for connection","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_GET_RESPONSE_SCHEMA","description":"Retrieves the response schema for a specified composio tool. this action fetches the complete response schema definition for any valid composio tool, returning it as a dictionary that describes the expected response structure.","inputSchema":{"type":"object","properties":{"tool":{"type":"string","description":"Name of the tool. For example: GITHUB_LIST_PULL_REQUESTS. You can find the relevant tool names using COMPOSIO_RETRIEVE_ACTIONS tool.\nExamples:\n  \"GITHUB_LIST_PULL_REQUESTS\"\n  \"JIRA_DELETE_PROJECT\""}},"required":["tool"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Get response schema","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_INITIATE_CONNECTION","description":"Initiate a connection to a toolkit with comprehensive authentication support. supports all authentication scenarios: 1. composio default oauth (no parameters needed) 2. custom oauth (user's client id/client secret) 3. api key/bearer token authentication 4. basic auth (username/password) 5. hybrid scenarios (oauth + connection fields like site name) 6. connection-only fields (subdomain, api key at connection level) 7. no authentication required automatically detects and validates auth config vs connection fields, provides helpful error messages for missing parameters.","inputSchema":{"type":"object","properties":{"parameters":{"type":"object","properties":{},"additionalProperties":true,"description":"\n        Authentication parameters for the connection. Structure depends on auth type:\n        \n        API Key Auth: {\"generic_api_key\": \"your_key\"}\n        Bearer Token: {\"bearer_token\": \"your_token\"} or {\"access_token\": \"your_token\"}\n        Basic Auth: {\"username\": \"user\", \"password\": \"pass\"}\n        Custom OAuth: {\"client_id\": \"your_id\", \"client_secret\": \"your_secret\"}\n        Connection Fields: {\"subdomain\": \"your_subdomain\", \"site_name\": \"your_site\"} \n        \n        Examples:\n        - Exa: {\"generic_api_key\": \"your_exa_api_key\"}\n        - GitHub (token): {\"access_token\": \"ghp_xxxxx\"}\n        - Google Super (OAuth): {\"client_id\": \"xxx.apps.googleusercontent.com\", \"client_secret\": \"GOCSPX-xxx\"}\n        - SharePoint (hybrid): {\"client_id\": \"your_id\", \"client_secret\": \"your_secret\", \"site_name\": \"your_site\"}\n        - Zendesk (connection only): {\"subdomain\": \"your_subdomain\"}\n        \n        Leave empty {} for default OAuth flow (if supported by toolkit).\n        Use get_required_parameters action to see exact parameter names and requirements.\n        "},"toolkit":{"type":"string","description":"Name of the toolkit to connect (e.g., 'gmail', 'exa', 'github', 'linear')"}},"required":["toolkit"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Initiate connection","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_LIST_TOOLKITS","description":"List all the available toolkits on composio with filtering options.","inputSchema":{"type":"object","properties":{"category":{"type":["string","null"],"default":null,"description":"Filter toolkits by category"},"min_tools":{"anyOf":[{"type":"integer"},{"type":"null"}],"default":null,"description":"Filter toolkits by minimum number of tools"},"name_filter":{"type":["string","null"],"default":null,"description":"Filter toolkits by name/slug"},"no_auth_only":{"type":"boolean","description":"Only return toolkits that don't require authentication","default":false},"size":{"anyOf":[{"type":"integer"},{"type":"null"}],"description":"Limit the number of results returned","default":10}},"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"List toolkits","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_LIST_TRIGGERS","description":"List available triggers and their configuration schemas.","inputSchema":{"type":"object","properties":{"toolkit_names":{"type":"array","items":{"type":"string"},"description":"List of toolkit names to filter triggers (optional), if not provided/empty, all triggers will be returned\nExamples:\n  \"gmail\"\n  \"github\""}},"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"List triggers","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_MANAGE_CONNECTIONS","description":"\nCreate or manage connections to user's apps. Returns a branded authentication link that works for OAuth, API keys, and all other auth types.\n\nCall policy:\n- First call COMPOSIO_SEARCH_TOOLS for the user’s query.\n- If COMPOSIO_SEARCH_TOOLS indicates there is no active connection for a toolkit, call COMPOSIO_MANAGE_CONNECTIONS with the exact toolkit name(s) returned.\n- Do not call COMPOSIO_MANAGE_CONNECTIONS if COMPOSIO_SEARCH_TOOLS returns no main tools and no related tools.\n- Toolkit names in toolkits must exactly match toolkit identifiers returned by COMPOSIO_SEARCH_TOOLS; never invent names.\n\nBehavior:\n- If a connection is active, the tool returns the connection details.\n- If a connection is not active, returns a authentication link (redirect_url) to create new connection.\n- If reinitiate_all is true, the tool forces reconnections for all toolkits, even if they already have active connections.\n\nResponse handling:\n- Always show the returned redirect_url as a FORMATTED MARKDOWN LINK to the user, and ask them to click on the link to finish authentication.\n- Begin executing tools only after the connection for that toolkit is confirmed Active.\n    ","inputSchema":{"type":"object","properties":{"toolkits":{"type":"array","items":{"type":"string"},"description":"List of toolkits to check or connect. Should be a valid toolkit returned by SEARCH_TOOLS (never invent one). If a toolkit is not connected, will initiate connection. Example: ['gmail', 'exa', 'github', 'outlook', 'reddit', 'googlesheets', 'one_drive']"},"reinitiate_all":{"type":"boolean","description":"Force reconnection for ALL toolkits in the toolkits list, even if they already have Active connections.\n              WHEN TO USE:\n              - You suspect existing connections are stale or broken.\n              - You want to refresh all connections with new credentials or settings.\n              - You're troubleshooting connection issues across multiple toolkits.\n              BEHAVIOR:\n              - Overrides any existing active connections for all specified toolkits and initiates new link-based authentication flows.\n              DEFAULT: false (preserve existing active connections)","default":false},"session_id":{"type":"string","description":"ALWAYS pass the session_id that was provided in the SEARCH_TOOLS response."}},"required":["toolkits"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Manage connections","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_MULTI_EXECUTE_TOOL","description":"\n  Fast and parallel tool executor for tools discovered through COMPOSIO_SEARCH_TOOLS. Use this tool to execute up to 20 tools in parallel across apps. Response contains structured outputs ready for immediate analysis - avoid reprocessing them via remote bash/workbench tools.\n\nPrerequisites:\n- Always use valid tool slugs and their arguments discovered through COMPOSIO_SEARCH_TOOLS. NEVER invent tool slugs or argument fields. ALWAYS pass arguments with the tool_slug in each tool.\n- Ensure Active connection statuses for the toolkits that are going to be executed through COMPOSIO_MANAGE_CONNECTIONS.\n- Only batch tools that are logically independent - no required ordering or dependencies between tools or their outputs.\n\nUsage guidelines:\n- Use this whenever a tool is discovered and has to be called, either as part of a multi-step workflow or as a standalone tool.\n- If COMPOSIO_SEARCH_TOOLS returns a tool that can perform the task, prefer calling it via this executor. Do not write custom API calls or ad-hoc scripts for tasks that can be completed by available Composio tools.\n- Prefer parallel execution: group independent tools into a single multi-execute call where possible.\n- Predictively set sync_response_to_workbench=true if the response may be large or needed for later scripting. It still shows response inline; if the actual response data turns out small and easy to handle, keep everything inline and SKIP workbench usage.\n- Responses contain structured outputs for each tool. RULE: Small data - process yourself inline; large data - process in the workbench.\n- ALWAYS include inline references/links to sources in MARKDOWN format directly next to the relevant text. Eg provide slack thread links alongside with summary, render document links instead of raw IDs.\n\n- CRITICAL: You MUST always include the 'memory' parameter - never omit it. Even if you think there's nothing to remember, include an empty object {} for memory.\n\nMemory Storage:\n- CRITICAL FORMAT: Memory must be a dictionary where keys are app names (strings) and values are arrays of strings. NEVER pass nested objects or dictionaries as values.\n- CORRECT format: {\"slack\": [\"Channel general has ID C1234567\"], \"gmail\": [\"John's email is john@example.com\"]}\n- Write memory entries in natural, descriptive language - NOT as key-value pairs. Use full sentences that clearly describe the relationship or information.\n- ONLY store information that will be valuable for future tool executions - focus on persistent data that saves API calls.\n- STORE: ID mappings, entity relationships, configs, stable identifiers.\n- DO NOT STORE: Action descriptions, temporary status updates, logs, or \"sent/fetched\" confirmations.\n- Examples of GOOD memory (store these):\n  * \"The important channel in Slack has ID C1234567 and is called #general\"\n  * \"The team's main repository is owned by user 'teamlead' with ID 98765\"\n  * \"The user prefers markdown docs with professional writing, no emojis\" (user_preference)\n- Examples of BAD memory (DON'T store these):\n  * \"Successfully sent email to john@example.com with message hi\"\n  * \"Fetching emails from last day (Sep 6, 2025) for analysis\"\n- Do not repeat the memories stored or found previously.\n","inputSchema":{"type":"object","properties":{"tools":{"type":"array","items":{"type":"object","properties":{"tool_slug":{"type":"string","minLength":1,"description":"The slug of the tool to execute - must be a valid slug from SEARCH_TOOLS response.\nExamples:\n  \"GMAIL_SEND_EMAIL\"\n  \"SLACK_SEND_MESSAGE\"\n  \"GITHUB_CREATE_ISSUE\""},"arguments":{"type":"object","properties":{},"additionalProperties":true,"description":"The arguments to pass to the tool. The argument schema is defined in the SEARCH_TOOLS response. Use exact field names and types; do not diverge from returned schemas.\nExamples:\n  {\"body\":\"This is a test\",\"subject\":\"Hello\",\"to\":\"test@gmail.com\"}\n  {\"channel\":\"#general\",\"text\":\"Hello from Composio!\"}\n  {\"body\":\"Description of the issue\",\"labels\":[\"bug\"],\"title\":\"Bug Report\"}"}},"required":["tool_slug","arguments"],"additionalProperties":false,"description":"MultiExecuteToolItem"},"minItems":1,"maxItems":50,"description":"List of tools to execute in parallel."},"thought":{"type":"string","description":"One-sentence, concise, high-level rationale (no step-by-step)."},"sync_response_to_workbench":{"type":"boolean","description":"Syncs the response to the remote workbench (for later scripting/processing) while still viewable inline. Predictively set true if the output may be large or need scripting; if it turns out small/manageable, skip workbench and use inline only. Default: false"},"memory":{"type":"object","properties":{},"additionalProperties":{"type":"array","items":{"type":"string","description":"Natural language memory string - e.g., \"John's user ID in Slack is 12345\", \"Venky's project MyProject has ID proj_abc123\""}},"description":"CRITICAL: Memory must be a dictionary with app names as keys and string arrays as values. NEVER use nested objects. Format: {\"app_name\": [\"string1\", \"string2\"]}. Store durable facts - stable IDs, mappings, roles, preferences. Exclude ephemeral data like message IDs or temp links. Use full sentences describing relationships. Always include this parameter."},"session_id":{"type":"string","description":"ALWAYS pass the session_id that was provided in the SEARCH_TOOLS response."},"current_step":{"type":"string","description":"Short enum for current step of the workflow execution. Eg FETCHING_EMAILS, GENERATING_REPLIES. Always include to keep execution aligned with the workflow."},"current_step_metric":{"type":"string","description":"Progress metrics for the current step - use to track how far execution has advanced. Format as a string \"done/total units\" - example \"10/100 emails\", \"0/n messages\", \"3/10 pages\""},"next_step":{"type":"string","description":"Enum for the next planned workflow step (may be the same as current_step). Eg \"GENERATING_SUMMARY\", \"FETCHING_EMAILS\"."}},"required":["tools","sync_response_to_workbench"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Multi Execute Composio Tools","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":true,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_REMOTE_BASH_TOOL","description":"\n  Execute bash commands in a REMOTE sandbox for file operations, data processing, and system tasks. Essential for handling large tool responses saved to remote files.\n  PRIMARY USE CASES:\n- Process large tool responses saved by COMPOSIO_MULTI_EXECUTE_TOOL to remote sandbox\n- File system operations, extract specific information from JSON with shell tools like jq, awk, sed, grep, etc.\n- Commands run from /home/user directory by default\n    ","inputSchema":{"type":"object","properties":{"command":{"type":"string","description":"The bash command to execute"},"session_id":{"type":"string","description":"ALWAYS pass the session_id that was provided in the SEARCH_TOOLS response."}},"required":["command"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Run bash commands","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":true,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_REMOTE_WORKBENCH","description":"\n  Process REMOTE FILES or script BULK TOOL EXECUTIONS using Python code IN A REMOTE SANDBOX. If you can see the data in chat, DON'T USE THIS TOOL.\n**ONLY** use this when processing **data stored in a remote file** or when scripting bulk tool executions.\n\nDO NOT USE\n- When the complete response is already inline/in-memory, or you only need quick parsing, summarization, or basic math.\n\nUSE IF\n- To parse/analyze tool outputs saved to a remote file in the sandbox or to script multi-tool chains there.\n- For bulk or repeated executions of known Composio tools (e.g., add a label to 100 emails).\n- To call APIs via proxy_execute when no Composio tool exists for that API.\n\nOUTPUTS\n- Returns a compact result or, if too long, artifacts under `/home/user/.code_out`.\n\nIMPORTANT CODING RULES:\n  1. Stepwise Execution: Split work into small steps. Save intermediate outputs in variables or temporary file in `/tmp/`. Call COMPOSIO_REMOTE_WORKBENCH again for the next step. This improves composability and avoids timeouts.\n  2. Notebook Persistence: This is a persistent Jupyter notebook cell: variables, functions, imports, and in-memory state from previous and future code executions are preserved in the notebook’s history and available for reuse. You also have a few helper functions available.\n  3. Parallelism & Timeout (CRITICAL): There is a hard timeout of 4 minutes so complete the code within that. Prioritize PARALLEL execution using ThreadPoolExecutor with suitable concurrency for bulk operations - e.g., call run_composio_tool or invoke_llm parallelly across rows to maximize efficiency.\n    3.1 If the data is large, split into smaller batches and call the workbench multiple times.\n  4. Checkpoints: Implement checkpoints (in memory or files) so that long runs can be resumed from the last completed step.\n  5. Schema Safety: Never assume the response schema for run_composio_tool if not known already from previous tools. To inspect schema, either run a simple request **outside** the workbench via COMPOSIO_MULTI_EXECUTE_TOOL or use invoke_llm helper.\n  6. LLM Helpers: Always use invoke_llm helper for summary, analysis, or field extraction on results. This is a smart LLM that will give much better results than any adhoc filtering.\n  7. Avoid Meta Loops: Do not use run_composio_tool to call COMPOSIO_MULTI_EXECUTE_TOOL or other COMPOSIO_* meta tools to avoid cycles. Only use it for app tools.\n  8. Pagination: Use when data spans multiple pages. Continue fetching pages with the returned next_page_token or cursor until none remains. Parallelize fetching pages if tool supports page_number.\n  9. No Hardcoding: Never hardcode data in code. Always load it from files or tool responses, iterating to construct intermediate or final inputs/outputs.\n  10. Code Correctness (CRITICAL): Code must be syntactically and semantically correct and executable.\n  11. If the final output is in a workbench file, use upload_local_file to download it - never expose the raw workbench file path to the user. Prefer to download useful artifacts after task is complete.\n\n\nENV & HELPERS:\n- Home directory: `/home/user`.\n- NOTE: Helper functions already initialized in the workbench - DO NOT import or redeclare them:\n    - \n`run_composio_tool(tool_slug: str, arguments: dict) -> tuple[Dict[str, Any], str]`: Execute a known Composio **app** tool (from COMPOSIO_SEARCH_TOOLS). Do not invent names; match the tool's input schema. Suited for loops/parallel/bulk over datasets.\n      i) run_composio_tool returns JSON with top-level \"data\". Parse carefully—structure may be nested.\n    \n    - \n`invoke_llm(query: str) -> tuple[str, str]`: Invoke an LLM for semantic tasks. Pass MAX 400000 characters in input.\n      i) NOTE Prompting guidance: When building prompts for invoke_llm, prefer f-strings (or concatenation) so literal braces stay intact. If using str.format, escape braces by doubling them ({{ }}).\n      ii) Define the exact JSON schema you want and batch items into smaller groups to stay within token limit.\n\n    - `proxy_execute(method, endpoint, toolkit, query_params=None, body=None, headers=None) -> tuple[Any, str]`: Call a toolkit API directly when no Composio tool exists. Only one toolkit can be invoked with proxy_execute per workbench call\n    - `web_search(query: str) -> tuple[str, str]`: Search the web for information.\n    - `upload_local_file(*file_paths) -> tuple[Dict[str, Any], str]`: Upload files to Composio S3/R2 storage. Use this to download any generated files/artifacts from the sandbox.\n    - `smart_file_extract(file_path: str, show_preview: bool = True) -> tuple[str, str]`: Process different file types and extract text content.\n    - Workbench comes with comprehensive Image Processing (PIL/Pillow, OpenCV, scikit-image), PyTorch ML libraries, Document and Report handling tools (pandoc, python-docx, pdfplumber, reportlab), and standard Data Analysis tools (pandas, numpy, matplotlib) for advanced visual, analytical, and AI tasks.\n  All helper functions return a tuple (result, error). Always check error before using result.\n\n## Python Helper Functions for LLM Scripting\n\n\n### run_composio_tool(tool_slug, arguments)\nExecutes a known Composio tool via backend API. Do NOT call COMPOSIO_* meta tools to avoid cyclic calls.\n\n    def run_composio_tool(tool_slug: str, arguments: Dict[str, Any]) -> tuple[Dict[str, Any], str]\n    # Returns: (tool_response_dict, error_message)\n    #   Success: ({\"data\": {actual_data}}, \"\") - Note the top-level data\n    #   Error:   ({}, \"error_message\") or (response_data, \"error_message\")\n\n    result, error = run_composio_tool(\"GMAIL_FETCH_EMAILS\", {\"max_results\": 1, \"user_id\": \"me\"})\n    if error:\n        print(\"GMAIL_FETCH_EMAILS error:\", error); return\n    email_data = result.get(\"data\", {})\n    print(\"Fetched:\", email_data)\n    \n\n\n### invoke_llm(query)\nCalls LLM for reasoning, analysis, and semantic tasks. Pass MAX 400k characters input.\n\n    def invoke_llm(query: str) -> tuple[str, str]\n    # Returns: (llm_response, error_message)\n\n    resp, error = invoke_llm(\"Summarize the key points from this data\")\n    if not error:\n      print(\"LLM:\", resp)\n\n    # Example: analyze tool response with LLM\n    tool_resp, err = run_composio_tool(\"GMAIL_FETCH_EMAILS\", {\"max_results\": 5, \"user_id\": \"me\"})\n    if not err:\n      parsed = tool_resp.get(\"data\", {})\n      resp, err2 = invoke_llm(f\"Analyze these emails and summarize: {parsed}\")\n      if not err2:\n        print(\"LLM Gmail Summary:\", resp)\n    # TIP: batch prompts to reduce LLM calls.\n    \n\n\n### proxy_execute(method, endpoint, toolkit, query_params=None, body=None, headers=None)\nDirect API call to a connected toolkit service.\n\n    def proxy_execute(\n        method: Literal[\"GET\",\"POST\",\"PUT\",\"DELETE\",\"PATCH\"],\n        endpoint: str,\n        toolkit: str,\n        query_params: Optional[Dict[str, str]] = None,\n        body: Optional[object] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> tuple[Any, str]\n    # Returns: (response_data, error_message)\n\n    # Example: GET request with query parameters\n    query_params = {\"q\": \"is:unread\", \"maxResults\": \"10\"}\n    data, error = proxy_execute(\"GET\", \"/gmail/v1/users/me/messages\", \"gmail\", query_params=query_params)\n    if not error:\n      print(\"Success:\", data)\n\n\n### web_search(query)\nSearches the web via Exa AI.\n\n    def web_search(query: str) -> tuple[str, str]\n    # Returns: (search_results_text, error_message)\n\n    results, error = web_search(\"latest developments in AI\")\n    if not error:\n        print(\"Results:\", results)\n\n\n### upload_local_file(*file_paths)\nUploads files to Composio S3/R2 storage. Single files upload directly, multiple files are auto-zipped.\nUse this when you need to upload/download any generated artifacts from the sandbox.\n\n    def upload_local_file(*file_paths) -> tuple[Dict[str, Any], str]\n    # Returns: (result_dict, error_string)\n    # Success: ({\"s3_url\": str, \"uploaded_file\": str, \"type\": str, \"id\": str, \"s3key\": str, \"message\": str}, \"\")\n    # Error: ({}, \"error_message\")\n\n    # Single file\n    result, error = upload_local_file(\"/path/to/report.pdf\")\n\n    # Multiple files (auto-zipped)\n    result, error = upload_local_file(\"/home/user/doc1.txt\", \"/home/user/doc2.txt\")\n\n    if not error:\n      print(\"Uploaded:\", result[\"s3_url\"])\n\n## Best Practices\n\n\n### Error-first pattern and Defensive parsing (print keys while narrowing)\n    res, err = run_composio_tool(\"GMAIL_FETCH_EMAILS\", {\"max_results\": 5})\n    if err:\n        print(\"error:\", err); return\n    if isinstance(res, dict):\n        print(\"res keys:\", list(res.keys()))\n        data = res.get(\"data\") or {}\n        print(\"data keys:\", list(data.keys()))\n        msgs = data.get(\"messages\") or []\n        print(\"messages count:\", len(msgs))\n        for m in msgs:\n            print(\"subject:\", m.get(\"subject\", \"<missing>\"))\n\n### Parallelize (4-min sandbox timeout)\nAdjust concurrency so all tasks finish within 4 minutes.\n\n    import concurrent.futures\n\n    MAX_CONCURRENCY = 10 # Adjust as needed\n\n    def send_bulk_emails(email_list):\n        def send_single(email):\n            result, error = run_composio_tool(\"GMAIL_SEND_EMAIL\", {\n                \"to\": email[\"recipient\"], \"subject\": email[\"subject\"], \"body\": email[\"body\"]\n            })\n            if error:\n                print(f\"Failed {email['recipient']}: {error}\")\n                return {\"status\": \"failed\", \"error\": error}\n            return {\"status\": \"sent\", \"data\": result}\n\n        results = []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_CONCURRENCY) as ex:\n            futures = [ex.submit(send_single, e) for e in email_list]\n            for f in concurrent.futures.as_completed(futures):\n                results.append(f.result())\n        return results\n\n    email_list = [{\"recipient\": f\"user{i}@example.com\", \"subject\": \"Test\", \"body\": \"Hello\"} for i in range(1000)]\n    results = send_bulk_emails(email_list)\n    \n\n\nGuidance: Ensure to peform the task with High Accuracy and Completeness. For large data, use parallel processing (ThreadPoolExecutor) and fewer batches in each call to maximise efficiency. Leverage invoke_llm for smart analysis whenever needed. NEVER hardcode data in code and NEVER run COMPOSIO_MULTI_EXECUTE_TOOL in the workbench.\n    ","inputSchema":{"type":"object","properties":{"code_to_execute":{"type":"string","description":"Python to run inside the persistent **remote Jupyter sandbox**. State (imports, variables, files) is preserved across executions. Keep code concise to minimize tool call latency. Avoid unnecessary comments.\nExamples:\n  \"import json, glob\\npaths = glob.glob(file_path)\\n...\"\n  \"result, error = run_composio_tool(tool_slug='SLACK_SEARCH_MESSAGES', arguments={'query': 'Rube'})\\nif error: return\\nmessages = result.get('data', {}).get('messages', [])\""},"thought":{"type":"string","description":"Concise objective and high-level plan (no private chain-of-thought). 1 sentence describing what the cell should achieve and why the sandbox is needed."},"session_id":{"type":"string","description":"ALWAYS pass the session_id that was provided in the SEARCH_TOOLS response."},"current_step":{"type":"string","description":"Short enum for current step of the workflow execution. Eg FETCHING_EMAILS, GENERATING_REPLIES. Always include to keep execution aligned with the workflow."},"current_step_metric":{"type":"string","description":"Progress metrics for the current step - use to track how far execution has advanced. Format as a string \"done/total units\" - example \"10/100 emails\", \"0/n messages\", \"3/10 pages\""},"next_step":{"type":"string","description":"Enum for the next planned workflow step (may be the same as current_step). Eg \"GENERATING_SUMMARY\", \"FETCHING_EMAILS\"."}},"required":["code_to_execute"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Execute Code remotely in work bench","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":true,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_SEARCH_TOOLS","description":"\n  MCP Server Info: COMPOSIO MCP connects 500+ apps—Slack, GitHub, Notion, Google Workspace (Gmail, Sheets, Drive, Calendar), Microsoft (Outlook, Teams), X, Figma, Web Search, Meta apps (WhatsApp, Instagram), TikTok, AI tools like Nano Banana & Veo3, and more—for seamless cross-app automation.\n  Use this MCP server to discover new tools and connect to apps.\n  ALWAYS call this tool first whenever a user mentions or implies an external app, service, or workflow—never say \"I don’t have access to X/Y app\" before calling it.\n\n  Tool Info: Extremely fast search tool to discover available MCP callable tools that can be used to solve a particular problem, user query or complete a task. It also returns a recommended execution plan and common pitfalls to help ensure a reliable execution.\n\n  Usage guidelines:\n  - Use this tool whenever kicking off a task. Post this, keep coming back to this tool to discover new tools.\n  - If the user pivots to a different use case in same chat, you MUST call this tool again with the new use case.\n  - Specify the use_case with a normalized description of the problem, query, or task. Be clear and precise so the system can find the most relevant tools. Queries can be simple single-app actions, or multiple queries can be linked to form complex cross-app workflows.\n  - Pass known_fields along with use_case as a list of key-value pairs to help the search provide tools to look up missing details (for example, finding channel_id from a given channel_name).\n\n  Example:\n  User query: \"send an email to John welcoming him and create a meeting invite for tomorrow\"\n  Search call: queries: [{use_case: \"send an email to someone\", known_fields: \"recipient_name: John\"}, {use_case: \"create a meeting invite\", known_fields: \"meeting_date: tomorrow\"}]\n\nResponse:\n  - The response lists toolkits (apps) and tools suitable for the task, along with their tool_slug, description, input schema / schemaRef, and related tools for prerequisites, alternatives, or next steps. NOTE: Tools with schemaRef instead of input_schema require you to call COMPOSIO_GET_TOOL_SCHEMAS first to load their full input_schema before use.\n  - The response also includes a detailed execution plan and common pitfalls for optimal execution. You MUST review and adapt this plan to your current context and follow it step-by-step to ensure reliable, accurate execution. Ignoring the plan can lead to unexpected failures.\n  - If a toolkit has an active connection, the response includes it along with any available current user information. If no active connection exists, you may initiate a new connection via COMPOSIO_MANAGE_CONNECTIONS by passing correct toolkit name.\n  - The response includes the current UTC time for reference. You can reference UTC time from the response if needed.\n  - The tools returned to you through this are to be called via COMPOSIO_MULTI_EXECUTE_TOOL. Make sure to specify the tool_slug and arguments for each tool execution properly.\n\n    - The response includes a memory parameter containing relevant information about the use case and the known fields that can be used to determine the flow of execution. Any user preferences in memory should be adhered to.\n\n\nSESSION: ALWAYS set this parameter, first for any workflow. Pass session: {generate_id: true} for new workflows OR session: {id: \"EXISTING_ID\"} to continue. ALWAYS use the returned session_id in ALL subsequent meta tool calls.\n    ","inputSchema":{"type":"object","properties":{"queries":{"type":"array","items":{"type":"object","properties":{"use_case":{"type":"string","description":"Provide a normalized English description of the complete use case to enable precise tool selection. Focus on the specific action and intended outcome. Include any specific apps if mentioned by user in each use_case. Do NOT include personal identifiers (names, emails, IDs) here — put those in known_fields.\nExamples:\n  \"send an email to someone\"\n  \"search jira issues with label\"\n  \"put issue details in a google sheet\"\n  \"post a message to slack with formatting\""},"known_fields":{"type":"string","description":"Provide known workflow inputs in English as comma-separated key:value pairs (not an array). Keep 2-3 short, structured items - stable identifiers, names, emails, or settings only. Omit if not relevant. No free-form or long text (messages, notes, descriptions).\nExamples:\n  \"channel_name:pod-sdk\"\n  \"channel_id:123\"\n  \"invitee_names:John,Maria, timezone:Asia/Kolkata\""}},"required":["use_case"],"additionalProperties":false},"minItems":1,"description":"List of structured search queries to process in parallel. Each query represents a specific use case or task. For multi-app or complex workflows, split them into smaller single-app queries for best accuracy. Each query returns 5-10 tools."},"session":{"type":"object","properties":{"id":{"type":"string","description":"Existing session identifier for the current workflow to reuse across calls."},"generate_id":{"type":"boolean","description":"Set to true for the first search call of a new usecase/workflow to generate a new session ID. When user pivots to a different task, set this true. If omitted or false with an existing session.id, the provided session ID will be reused."}},"additionalProperties":false,"description":"Session context for correlating meta tool calls within a workflow. Always pass this parameter. Use {generate_id: true} for new workflows or {id: \"EXISTING_ID\"} to continue existing workflows."}},"required":["queries"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Search Composio Tools","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_WAIT_FOR_CONNECTION","description":"\nWait for the user to complete authentication AFTER you have given them an auth URL from COMPOSIO_MANAGE_CONNECTIONS. Use this **immediately after** sharing the auth URL so you can automatically continue once the connection is established — without waiting for the user to manually come back and say they’re done. Make sure you have printed the auth URL, DO NOT call this tool without printing auth URL first.\n    This ensures a smooth, uninterrupted flow and a better user experience.\n  You NEED NOT wait if there is no auth URL in the response of COMPOSIO_MANAGE_CONNECTIONS like in cases you ask user for  api_key, client_id or client_secret.\n    Input params <toolkits: list of toolkit names>, <mode (any / all) : wait for ANY or ALL connections to reach success/failed state (default: any) >\n    Output params <connection statuses>\n    Example:\n    input:\n    toolkits: [gmail, outlook]\n    mode: [any]\n    output: {\n      gmail: {\n        status: [connected]\n      },\n      outlook: {\n        status: [initiated]\n      }\n    }\n    ","inputSchema":{"type":"object","properties":{"mode":{"type":"string","enum":["any","all"],"description":"Wait for ANY connection or ALL connections to reach success/failed state (default: any)","default":"any"},"timeout_seconds":{"type":"integer","minimum":1,"maximum":600,"description":"Maximum time to wait in seconds (default: 300, max: 600)","default":300},"toolkits":{"type":"array","items":{"type":"string"},"minItems":1,"description":"List of toolkit slugs to wait for"},"session_id":{"type":"string","description":"ALWAYS pass the session_id that was provided in the SEARCH_TOOLS response."}},"required":["toolkits"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Wait for connection","scopes":[],"readOnlyHint":true,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_CREATE_UPDATE_RECIPE","description":"\n    Convert the executed workflow into a notebook.\n    If recipe_id is NOT provided, then create a new one.\n    Else, update the existing one.\n\n    \n    This tool allows you to:\n\n    1. Save the full workflow execution as a reusable notebook\n    2. If I share the notebook with others (when published) it can be used by passing correct environment variables and it executes the whole workflow from beginning of this session\n    3. You should generate input json schema of this notebook based on the executed workflow, so that other users of this published notebook can pass their own valid inputs\n    4. You should generate the code of this notebook based on the executed workflow, please see below for more instructions on how to generate the code for the notebook\n    5. Similarly please generate good output json schema for this notebook, so that other users of this published notebook know how to consume the response\n    6. The notebook should take the input from environment variables using os.environ.get(). The user of this notebook will pass each key of the input json schema as an env var\n\n    WHEN TO USE\n    - Only run this tool when the workflow is completed and successful or if the user explicitly asked to run this tool\n\n    DO NOT USE\n    - When the workflow is still being processed, or not yet completed and the user explicitly didn't ask to run this tool\n\n    IMPORTANT CODING RULES:\n    1. Single Execution: Please generate the code for the full notebook that can be executed in a single invocation\n    2. Schema Safety: Never assume the response schema for run_composio_tool if not known already from previous tools. To inspect schema, either run a simple request **outside** the workbench via COMPOSIO_MULTI_EXECUTE_TOOL or use invoke_llm helper.\n    3. Parallelism & Timeout (CRITICAL): There is a hard timeout of 4 minutes so complete the code within that. Prioritize PARALLEL execution using ThreadPoolExecutor with suitable concurrency for bulk operations - e.g., call run_composio_tool or invoke_llm parallelly across rows to maximize efficiency.\n    4. LLM Helpers: You should always use invoke_llm helper for summary, analysis, or field extraction on results. This is a smart LLM that will give much better results than any adhoc filtering.\n    5. Avoid Meta Loops: Do not use run_composio_tool to call COMPOSIO_MULTI_EXECUTE_TOOL or other COMPOSIO_* meta tools to avoid cycles. Only use it for app tools.\n    6. Pagination: Use when data spans multiple pages. Continue fetching pages with the returned next_page_token or cursor until none remains. Parallelize fetching pages if tool supports page_number.\n    7. No Hardcoding: Never hardcode data in code. Always load it from files or tool responses, iterating to construct intermediate or final inputs/outputs.\n    8. No Hardcoding: Please do NOT hardcode any PII related information like email / name / home address / social security id or anything like that. This is very risky\n    9. NEVER HARDCODE CONTENT (CRITICAL): For ANY content generation use case, you MUST use invoke_llm helper instead of hardcoding. This includes but is not limited to: social media posts (Twitter, LinkedIn, Instagram, Facebook, Reddit, etc.), blog posts, articles, email content, SEO reports, market research, jokes, stories, creative writing, product descriptions, news summaries, documentation, or ANY text content that should be unique, personalized, or contextual. Always use invoke_llm with a specific prompt to generate fresh content every time.\n    10. Dynamic Content Generation: Structure your code like this for content generation: content_prompt = f\"Generate a [specific type] about [topic] that [requirements]\" then generated_content, error = invoke_llm(content_prompt). Every execution should produce different, contextually appropriate content.\n    11. Code Correctness (CRITICAL): Code must be syntactically and semantically correct and executable.\n    12. Please ensure that your code takes input in the form of input json schema via environment variables and stores the output at last command in the form of output json schema\n    13. Your notebook must read each key of the input json schema in your code from environment variables using os.environ.get()\n    14. Please always end the code with just \"output\" command following the output json schema so that the notebook actually shows it. DO NOT PRINT IT AT ANY COST\n    15. Please only take needed args as input in input json schema and only give the needed data in the output. This is to keep the json schemas and notebook simple\n    16. Please do NOT default inputs in os.environ.get() code to any sensitive / PII information. This is very risky. It is fine to default for non-sensitive inputs\n    17. Debugging (CRITICAL): For every print statement in your code, please prefix it with the time at which it is being executed. This will help me investigate latency related stuff\n    18. If there are any errors while running, please throw the error so that the person running the notebook can see and fix it\n    19. FAIL LOUDLY (CRITICAL): If you expect data but get 0 results, raise Exception immediately. NEVER silently continue or create empty outputs. Recovery loop will fix the code - don't hide issues. Example: if len(items) == 0: raise Exception(\"Expected data but got none\")\n    20. NESTED DATA (CRITICAL): APIs often double-nest data. Always extract: data = result.get(\"data\", {}); if \"data\" in data: data = data[\"data\"]. Try flexible field names: item.get(\"id\") or item.get(\"channel_id\")\n\n    IMPORTANT SCHEMA RULES:\n    1. Keep input schema simple - ask only for parameters users would want to vary between runs\n    2. Do not ask for large inputs. Use invoke_llm helper to generate large content in the code\n    3. HUMAN-FRIENDLY INPUTS (CRITICAL):\n       - ✓ Ask for: channel_name, google_sheet_url, repo_name, email_address\n       - ✗ Never ask for: channel_id, spreadsheet_id, document_id, user_id\n       - Extract IDs in code: use FIND/SEARCH tools to convert names/URLs to IDs\n       - For URLs: extract IDs in code with regex (e.g., spreadsheet_id from google_sheet_url)\n    4. REQUIRED vs OPTIONAL: Mark as required only if it's specific to the user's workflow and would change every run. Generic settings should be optional with sensible defaults\n    5. Identify what varies between runs: channel name, date range, search terms = required. Sheet tab name, row limits, formatting = optional\n    6. [CRITICAL]: Use search/find tools in code to convert human inputs (names/URLs) to IDs before calling other tools\n\n    IMPORTANT RULES ON DEFAULTS FOR REQUIRED PARAMETERS :\n    1. Please ensure that default parameters are provided for all required inputs in the input schema (even if it is PII related info and specific to users) from your context\n    2. If there is no value available from context, please assume the default to be empty string. Do NOT hallucinate a random name or id\n    3. Ensure that there is no type mismatch in the default parameters with the input schema. For example, if the input schema is a string, the default should be a string, if the input schema is a number, the default should be a number, etc.\n    4. If user doesn't provide any input for this workflow, we will use these parameters. Otherwise the workflow will break\n    5. The values of these parameters should reflect the workflow execution and specific to the user creating / updating the recipe and not be random\n\n    ENV & HELPERS:\n    You can get the list of helper functions and their details from COMPOSIO_REMOTE_WORKBENCH tool's description.\n\n    NOTE: Please do not forget to read the environment variables for input and end the notebook with just output (DO NOT PRINT)\n  \n\n    ","inputSchema":{"type":"object","properties":{"recipe_id":{"type":"string","description":"Recipe id to update (optional). If not provided, will create a new recipe\nExample: \"rcp_rBvLjfof_THF\""},"name":{"type":"string","description":"Name for the notebook / recipe. Please keep it short (ideally less than five words)\nExamples:\n  \"Get Github Contributors\"\n  \"Send Weekly Gmail Report\"\n  \"Analyze Slack Messages\""},"description":{"type":"string","description":"Description for the notebook / recipe\nExamples:\n  \"Get contributors from Github repository and save to Google Sheet\"\n  \"Send weekly Gmail report to all users by sending email to each user\"\n  \"Analyze Slack messages from a particular channel and send summary to all users\""},"output_schema":{"type":"object","properties":{},"additionalProperties":true,"description":"Expected output json schema of the Notebook / Recipe. If the schema has array, please ensure it has \"items\" in it, so we know what kind of array it is. If the schema has object, please ensure it has \"properties\" in it, so we know what kind of object it is\nExample: {\"properties\":{\"contributors_count\":{\"description\":\"Count of contributors to Github repository\",\"name\":\"contributors_count\",\"type\":\"number\"},\"sheet_id\":{\"description\":\"ID of the sheet\",\"name\":\"sheet_id\",\"type\":\"string\"},\"sheet_updated\":{\"description\":\"Is the sheet updated?\",\"name\":\"sheet_updated\",\"type\":\"boolean\"},\"contributor_profiles\":{\"name\":\"contributor_profiles\",\"type\":\"array\",\"items\":{\"type\":\"object\"},\"description\":\"Profiles of top 10 contributors\"}},\"type\":\"object\"}"},"input_schema":{"type":"object","properties":{},"additionalProperties":false,"description":"Expected input json schema for the Notebook / Recipe. Please keep the schema simple, avoid nested objects and arrays. Types of all input fields should be string only. Each key of this schema will be a single environment variable input to your Notebook\nExample: {\"properties\":{\"repo_owner\":{\"description\":\"GitHub repository owner username\",\"name\":\"repo_owner\",\"required\":true,\"type\":\"string\"},\"repo_name\":{\"description\":\"GitHub repository name\",\"name\":\"repo_name\",\"required\":true,\"type\":\"string\"},\"google_sheet_url\":{\"description\":\"Google Sheet URL (e.g., https://docs.google.com/spreadsheets/d/SHEET_ID/edit)\",\"name\":\"google_sheet_url\",\"required\":true,\"type\":\"string\"},\"sheet_tab\":{\"description\":\"Sheet tab name to write data to\",\"name\":\"sheet_tab\",\"required\":false,\"type\":\"string\"}},\"type\":\"object\"}"},"workflow_code":{"type":"string","description":"The Python code that implements the workflow, generated by the LLM based on the executed workflow. Should include all necessary imports, tool executions (via run_composio_tool), and proper error handling. Notebook should always end with output cell (not print)\nExample: \"import os\\nimport re\\nfrom datetime import datetime\\n\\nprint(f\\\"[{datetime.utcnow().isoformat()}] Starting workflow\\\")\\n\\nrepo_owner = os.environ.get(\\\"repo_owner\\\")\\nrepo_name = os.environ.get(\\\"repo_name\\\")\\ngoogle_sheet_url = os.environ.get(\\\"google_sheet_url\\\")\\n\\nif not repo_owner or not repo_name or not google_sheet_url:\\n    raise ValueError(\\\"repo_owner, repo_name, and google_sheet_url are required\\\")\\n\\n# Extract spreadsheet ID from URL\\nif \\\"docs.google.com\\\" in google_sheet_url:\\n    match = re.search(r'/d/([a-zA-Z0-9-_]+)', google_sheet_url)\\n    spreadsheet_id = match.group(1) if match else google_sheet_url\\nelse:\\n    spreadsheet_id = google_sheet_url\\n\\nprint(f\\\"[{datetime.utcnow().isoformat()}] Fetching contributors from {repo_owner}/{repo_name}\\\")\\ngithub_result, error = run_composio_tool(\\n    \\\"GITHUB_LIST_CONTRIBUTORS\\\",\\n    {\\\"owner\\\": repo_owner, \\\"repo\\\": repo_name}\\n)\\n\\nif error:\\n    raise Exception(f\\\"Failed to fetch contributors: {error}\\\")\\n\\n# Handle nested data\\ndata = github_result.get(\\\"data\\\", {})\\nif \\\"data\\\" in data:\\n    data = data[\\\"data\\\"]\\n\\ncontributors = data.get(\\\"contributors\\\") or data if isinstance(data, list) else []\\n\\nif len(contributors) == 0:\\n    raise Exception(f\\\"No contributors found for {repo_owner}/{repo_name}\\\")\\n\\nprint(f\\\"[{datetime.utcnow().isoformat()}] Found {len(contributors)} contributors\\\")\\n\\n# Process data\\nrows = []\\nfor contributor in contributors:\\n    rows.append([\\n        contributor.get(\\\"login\\\"),\\n        contributor.get(\\\"email\\\", \\\"\\\"),\\n        contributor.get(\\\"location\\\", \\\"\\\"),\\n        contributor.get(\\\"contributions\\\", 0)\\n    ])\\n\\nprint(f\\\"[{datetime.utcnow().isoformat()}] Adding {len(rows)} rows to sheet\\\")\\nsheets_result, sheets_error = run_composio_tool(\\n    \\\"GOOGLESHEETS_APPEND_DATA\\\",\\n    {\\n        \\\"spreadsheet_id\\\": spreadsheet_id,\\n        \\\"range\\\": \\\"A1\\\",\\n        \\\"values\\\": rows\\n    }\\n)\\n\\nif sheets_error:\\n    raise Exception(f\\\"Failed to update sheet: {sheets_error}\\\")\\n\\nprint(f\\\"[{datetime.utcnow().isoformat()}] Workflow completed\\\")\\n\\noutput = {\\n    \\\"contributors_count\\\": len(contributors),\\n    \\\"sheet_updated\\\": True,\\n    \\\"spreadsheet_id\\\": spreadsheet_id\\n}\\noutput\""},"defaults_for_required_parameters":{"type":"object","properties":{},"additionalProperties":false,"description":"\nDefaults for required parameters of the notebook / recipe. We store those PII related separately after encryption.\nPlease ensure that the parameters you provide match the input schema for the recipe and all required inputs are covered. Fine to ignore optional parameters\nExample: {\"repo_owner\":\"composiohq\",\"repo_name\":\"composio\",\"sheet_id\":\"1234567890\"}"}},"required":["name","description","output_schema","input_schema","workflow_code"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Create / Update Recipe from Workflow","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_UPSERT_SKILL","description":"\n    Convert the executed workflow into a skill using Python Pydantic code.\n    The skill_slug parameter is required. If a skill with the provided slug already exists, a new version will be created.\n    If the slug does not exist, a new skill will be created.\n\n    \n    This tool allows you to:\n\n    1. Save the executed workflow as a reusable skill using Python Pydantic code\n    2. The skill_slug parameter is required. If a skill with the provided slug already exists, a new version will be created. If the slug does not exist, a new skill will be created.\n    3. Skills are defined using Python Pydantic models that extend ComposioSkill base class\n    4. You should generate the Python Pydantic code for the skill based on the executed workflow, please see below for more instructions on how to generate the code for the skill\n    5. The skill code must define request and response Pydantic models and implement the execute method\n\n    WHEN TO USE\n    - Only run this tool when the workflow is completed and successful or if the user explicitly asked to run this tool\n\n    DO NOT USE\n    - When the workflow is still being processed, or not yet completed and the user explicitly didn't ask to run this tool\n\n    IMPORTANT CODING RULES:\n    1. Single Execution: Please generate the code for the full skill that can be executed in a single invocation\n    2. Schema Safety: Never assume the response schema for run_composio_tool if not known already from previous tools. To inspect schema, either run a simple request **outside** the workbench via COMPOSIO_MULTI_EXECUTE_TOOL or use invoke_llm helper.\n    3. Parallelism & Timeout (CRITICAL): There is a hard timeout of 4 minutes so complete the code within that. Prioritize PARALLEL execution using ThreadPoolExecutor with suitable concurrency for bulk operations - e.g., call run_composio_tool or invoke_llm parallelly across rows to maximize efficiency.\n    4. LLM Helpers: You should always use invoke_llm helper for summary, analysis, or field extraction on results. This is a smart LLM that will give much better results than any adhoc filtering.\n    5. Avoid Meta Loops: Do not use run_composio_tool to call COMPOSIO_MULTI_EXECUTE_TOOL or other COMPOSIO_* meta tools to avoid cycles. Only use it for app tools.\n    6. Pagination: Use when data spans multiple pages. Continue fetching pages with the returned next_page_token or cursor until none remains. Parallelize fetching pages if tool supports page_number.\n    7. No Hardcoding: Never hardcode data in code. Always load it from files or tool responses, iterating to construct intermediate or final inputs/outputs.\n    8. No Hardcoding: Please do NOT hardcode any PII related information like email / name / home address / social security id or anything like that. This is very risky\n    9. NEVER HARDCODE CONTENT (CRITICAL): For ANY content generation use case, you MUST use invoke_llm helper instead of hardcoding. This includes but is not limited to: social media posts (Twitter, LinkedIn, Instagram, Facebook, Reddit, etc.), blog posts, articles, email content, SEO reports, market research, jokes, stories, creative writing, product descriptions, news summaries, documentation, or ANY text content that should be unique, personalized, or contextual. Always use invoke_llm with a specific prompt to generate fresh content every time.\n    10. Dynamic Content Generation: Structure your code like this for content generation: content_prompt = f\"Generate a [specific type] about [topic] that [requirements]\" then generated_content, error = invoke_llm(content_prompt). Every execution should produce different, contextually appropriate content.\n    11. Code Correctness (CRITICAL): Code must be syntactically and semantically correct and executable.\n    12. Skill Structure: The skill code must follow this structure:\n        - Import required modules: from pydantic import BaseModel, Field; from skills.base import ComposioSkill\n        - Define Request model: class SkillRequest(BaseModel) with Field descriptions\n        - Define Response model: class SkillResponse(BaseModel) with Field descriptions\n        - Define Skill class: class SkillName(ComposioSkill[SkillRequest, SkillResponse]) with name attribute and execute method\n        - The execute method should use self.run_composio_tool() to call tools and return SkillResponse instance\n        - IMPORTANT: The skill class must have a name attribute that is a slug-type identifier (lowercase, underscores, no spaces). Example: name = \"weather_lookup\" or name = \"github_repo_analyzer\". This name will be used as the skill slug identifier.\n    13. Pydantic Models: Use proper Field descriptions and types. Request model should have all input parameters with Field(..., description=\"...\")\n    14. Response Model: Should match the expected output structure with proper Field descriptions\n    15. Tool Execution: Use self.run_composio_tool(tool_slug, arguments) to execute tools within the skill\n    16. Error Handling: Handle errors appropriately and return meaningful error messages\n    17. Debugging (CRITICAL): For every print statement in your code, please prefix it with the time at which it is being executed. This will help me investigate latency related stuff\n    18. If there are any errors while running, please throw the error so that the person running the skill can see and fix it\n    19. FAIL LOUDLY (CRITICAL): If you expect data but get 0 results, raise Exception immediately. NEVER silently continue or create empty outputs. Recovery loop will fix the code - don't hide issues. Example: if len(items) == 0: raise Exception(\"Expected data but got none\")\n    20. NESTED DATA (CRITICAL): APIs often double-nest data. Always extract: data = result.get(\"data\", {}); if \"data\" in data: data = data[\"data\"]. Try flexible field names: item.get(\"id\") or item.get(\"channel_id\")\n\n    IMPORTANT SCHEMA RULES:\n    1. Keep request model simple - include only parameters users would want to vary between runs\n    2. Do not ask for large inputs. Use invoke_llm helper to generate large content in the code\n    3. HUMAN-FRIENDLY INPUTS (CRITICAL):\n       - ✓ Ask for: channel_name, google_sheet_url, repo_name, email_address\n       - ✗ Never ask for: channel_id, spreadsheet_id, document_id, user_id\n       - Extract IDs in code: use FIND/SEARCH tools to convert names/URLs to IDs\n       - For URLs: extract IDs in code with regex (e.g., spreadsheet_id from google_sheet_url)\n    4. REQUIRED vs OPTIONAL: Mark as required only if it's specific to the user's workflow and would change every run. Generic settings should be optional with sensible defaults\n    5. Identify what varies between runs: channel name, date range, search terms = required. Sheet tab name, row limits, formatting = optional\n    6. [CRITICAL]: Use search/find tools in code to convert human inputs (names/URLs) to IDs before calling other tools\n\n    ENV & HELPERS:\n    Skills inherit from ComposioSkill and have access to these helper methods:\n\n    1. self.run_composio_tool(tool_slug: str, params: Dict) -> Tuple[Dict, Optional[str]]\n       - Execute a Composio tool\n       - Returns: (result dict, error string or None)\n\n    2. self.invoke_llm(prompt: str) -> Tuple[str, Optional[str]]\n       - Invoke an LLM for semantic analysis, reasoning, summarization, and other generic tasks\n       - Returns: (completion text, error string or None)\n\n    3. self.web_search(query: str) -> Tuple[str, Optional[str]]\n       - Perform web search\n       - Returns: (answer text, error string or None)\n\n    NOTE: The skill code must be valid Python Pydantic code that extends ComposioSkill base class.\n  \n\n    ","inputSchema":{"type":"object","properties":{"skill_slug":{"type":"string","description":"Skill slug identifier (required). If the slug exists, it will be updated (a new version will be created). If the slug does not exist, a new skill will be created. The slug is a unique identifier for the skill (e.g., \"skill_weather_lookup\"). Maximum length is 32 characters.\nExamples:\n  \"skill_weather_lookup\"\n  \"skill_github_repo_analyzer\""},"name":{"type":"string","description":"Name for the notebook / recipe. Please keep it short (ideally less than five words)\nExamples:\n  \"Get Github Contributors\"\n  \"Send Weekly Gmail Report\"\n  \"Analyze Slack Messages\""},"description":{"type":"string","description":"Description for the notebook / recipe\nExamples:\n  \"Get contributors from Github repository and save to Google Sheet\"\n  \"Send weekly Gmail report to all users by sending email to each user\"\n  \"Analyze Slack messages from a particular channel and send summary to all users\""},"skill_code":{"type":"string","description":"The Python Pydantic code that implements the skill, generated by the LLM based on the executed workflow. Should include Pydantic models for request and response, and a skill class extending ComposioSkill with an execute method.\nExample: \"from pydantic import BaseModel, Field\\nfrom skills.base import ComposioSkill\\n\\nclass WeatherRequest(BaseModel):\\n    location: str = Field(..., description=\\\"Location to get weather for\\\")\\n\\nclass WeatherResponse(BaseModel):\\n    location: str = Field(..., description=\\\"Location queried\\\")\\n    temperature: float = Field(..., description=\\\"Temperature in Kelvin\\\")\\n    weather_description: str = Field(..., description=\\\"Weather description\\\")\\n\\nclass WeatherLookupSkill(ComposioSkill[WeatherRequest, WeatherResponse]):\\n    name = \\\"weather_lookup\\\"\\n\\n    def execute(self, request):\\n        result, error = self.run_composio_tool(\\n            \\\"WEATHERMAP_WEATHER\\\",\\n            {\\\"location\\\": request.location}\\n        )\\n        weather_data = result.get(\\\"data\\\", {})\\n        main = weather_data.get(\\\"main\\\", {})\\n        weather = weather_data.get(\\\"weather\\\", [{}])[0]\\n        return WeatherResponse(\\n            location=request.location,\\n            temperature=main.get(\\\"temp\\\", 0.0),\\n            weather_description=weather.get(\\\"description\\\", \\\"unknown\\\")\\n        )\""}},"required":["skill_slug","name","description","skill_code"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Create / Update Skill from Workflow","scopes":[],"readOnlyHint":false,"openWorldHint":true,"destructiveHint":false,"idempotentHint":false,"updateHint":false}},{"name":"COMPOSIO_GET_TOOL_SCHEMAS","description":"Retrieve input schemas for tools by slug. Returns complete parameter definitions required to execute each tool. Make sure to call this tool whenever the response of COMPOSIO_SEARCH_TOOLS does not provide a complete schema for a tool - you must never invent or guess any input parameters.","inputSchema":{"type":"object","properties":{"tool_slugs":{"type":"array","items":{"type":"string","minLength":1},"description":"List of tool slugs to retrieve schemas for. Each slug MUST be a valid tool slug previously returned by COMPOSIO_SEARCH_TOOLS.\nExamples:\n  [\"GMAIL_SEND_EMAIL\"]\n  [\"GMAIL_SEND_EMAIL\",\"SLACK_SEND_MESSAGE\"]"},"session_id":{"type":"string","description":"ALWAYS pass the session_id that was provided in the SEARCH_TOOLS response."}},"required":["tool_slugs"],"additionalProperties":false,"$schema":"http://json-schema.org/draft-07/schema#"},"annotations":{"title":"Get Tool Schemas","scopes":[],"readOnlyHint":true,"openWorldHint":false,"destructiveHint":false,"idempotentHint":false,"updateHint":false}}]},"jsonrpc":"2.0","id":2}

